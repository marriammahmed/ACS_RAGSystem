# Project Documentation: Local RAG with Ollama (Text + Media + OCR + Video Captions)

Backend file: `backend.py`  
UI file: `ui2.py`  

---

## 0. Prerequisites and Setup

This project runs fully **offline/local** using:

- **Ollama** for LLM + embeddings  
- **ChromaDB** for persistent vector storage  
- **CLIP embeddings** for images/video frames  
- **EasyOCR** for extracting text from images  
- **LLaVA** (via Ollama) for describing video frames and making them searchable  

---

## 0.1 Required software

- Python **3.10+**
- pip
- Ollama installed and running
- (Optional) Git

---

## 0.2 Install Ollama and pull required models

You need these models for the full system:

```bash
ollama pull llama3
ollama pull nomic-embed-text
ollama pull llava
```

Defaults from config:

- `llm_model = "llama3"`
- `embedding_model = "nomic-embed-text"`
- `vision_llm_model = "llava"`

---

## 0.3 Create + activate a virtual environment (recommended)

```bash
python -m venv .venv
```

Windows PowerShell:

```powershell
.venv\Scripts\Activate.ps1
```

macOS/Linux:

```bash
source .venv/bin/activate
```

---

## 0.4 Install Python dependencies

```bash
pip install streamlit chromadb requests beautifulsoup4 pillow opencv-python sentence-transformers langchain langchain-community langchain-core langchain-text-splitters langchain-ollama pypdf easyocr numpy
```

---

## 0.5 Run the app

```bash
streamlit run ui2.py
```

---

## 0.6 Folder structure

Important folders created during runtime:

- `./data`
  - stores uploaded files
- `./chroma_db`
  - persistent ChromaDB vector storage
- `./data/video_frames`
  - extracted video frame images saved here

---

## 1. Project Overview

This project is a local **Retrieval-Augmented Generation (RAG)** system that lets you:

- Upload PDFs / txt / md  
- Upload images and videos  
- Ingest links (web scraping)  
- Ask questions in chat  
- Get answers grounded in your stored sources  
- See explainability details (chunks, sources, media retrieval)  
- Evaluate answers for hallucination + quality

---

## 2. Supported Inputs

### Text Inputs

- `.pdf`
- `.txt`
- `.md`
- URLs (`https://...`)

### Media Inputs

- Images: `.png`, `.jpg`, `.jpeg`
- Videos: `.mp4`, `.mov`, `.mkv`

---

## 3. Output Features

### System outputs

- Answer generated by Ollama model
- Source list used to answer the question
- Retrieved text chunks + similarity distances
- Retrieved related media (images/video frames)
- Answer evaluation metrics

---

## 4. High-Level Architecture

Upload/Link → Parse → Chunk → Embed → Store → Query → Retrieve → Prompt → Answer

This system uses **two vector DB collections**:

### Text Collection

- Stored in `rag_text`
- Uses **Ollama text embeddings** (`nomic-embed-text`)
- Stores:
  - PDF/TXT/MD text chunks
  - URL scraped text chunks
  - Image OCR text chunks
  - Video frame captions

### Media Collection

- Stored in `rag_media`
- Uses **CLIP embeddings**
- Stores:
  - Image embeddings
  - Video frame embeddings

---

## Backend Documentation (`backend.py`)

---

## 5. Configuration

### DEFAULT_CONFIG

```python
DEFAULT_CONFIG = {
    "data_folder": "./data",
    "persist_directory": "./chroma_db",
    "embedding_model": "nomic-embed-text",
    "llm_model": "llama3",
    "vision_llm_model": "llava",
    "top_k_retrieval": 5,
    "chunk_size": 500,
    "chunk_overlap": 50,
    "text_collection": "rag_text",
    "media_collection": "rag_media",
    "allowed_extensions": [...],
    "video_frames_folder": "./data/video_frames",
    "video_frame_step": 60,
    "video_max_frames": 30,
}
```

---

## 6. Prompting

### RAG_PROMPT_TEMPLATE

The model is forced to answer using only retrieved text:

Rules:

- only use provided context  
- refuse if missing info  
- refusal message must be **exactly**:

> `"I don't have enough information to answer this question."`

---

## 7. Utility Functions

### `ensure_folder(path)`

Creates folders if missing.

### `safe_filename(name)`

Prevents directory traversal by replacing `/` and `\`.

### `extract_text_from_url(url)`

Scrapes webpage visible text.

### `load_text_from_file(path)`

Loads PDFs and txt/md.

### `chunk_text(text, chunk_size, chunk_overlap)`

Splits text into chunks using RecursiveCharacterTextSplitter.

### `embed_clip_image(pil_img)`

Embeds images using CLIP.

---

## 8. OCR Support

### `extract_text_easyocr(image_path)`

Uses `easyocr` to extract text from images (English + German).

OCR chunks are stored in the **text DB**, so you can search text inside images.

---

## 9. Video Frame Captions

### `caption_frame_with_ollama(llm, pil_img)`

Uses a vision-capable Ollama model (`llava`) to caption a frame.

Captions are chunked + stored in the **text DB**, so video content becomes searchable.

---

## 10. Vector DB Classes

### TextVectorDB

Stores text chunks in ChromaDB.

### MediaVectorDB

Stores media embeddings in ChromaDB using CLIP.

---

## 11. RAGSystem Class

### Initialization

Creates:

- TextVectorDB
- MediaVectorDB
- Chat model (`llm_model`)
- Vision Chat model (`vision_llm_model`)

---

## 11.1 Ingestion Methods

### `ingest_text_source(source)`

Ingests URLs or local text files.

### `ingest_image(image_path)`

Stores:

- image CLIP embedding (media DB)
- OCR text chunks (text DB)

### `ingest_video(video_path)`

Stores:

- video frame CLIP embeddings (media DB)
- LLaVA captions for each frame (text DB)

---

## 11.2 Retrieval + Answering

### `retrieve_text(question)`

Text semantic search.

### `retrieve_media(question)`

Media semantic search.

### `answer(question)`

Refuses only if **no text and no media** found.

---

## 11.3 No-RAG Mode

### `noRAGAnswer(question)`

Calls the LLM directly without retrieval.

---

## 11.4 Clearing + Deleting

### `delete_source_everywhere(source)`

Deletes source from both DBs.

### `clear_all()`

Deletes both collections, deletes `./chroma_db`, and rebuilds empty DBs.

---

## 12. Evaluation + Metrics

Includes:

- `evaluate_rag()` basic overlap grounding
- `RAG_Evaluator` for faithfulness/relevance/hallucination detection + composite score

---

## UI Documentation (`ui2.py`)

---

## 13. Streamlit UI Overview

Includes:

- Upload files + ingest
- Ingest links
- Manage sources deletion
- Chat interface
- Answer details + retrieval transparency
- Answer quality metrics
- RAG toggle

---

## 14. UI State

Session state stores:

- system (RAGSystem)
- evaluator (RAG_Evaluator)
- messages
- last_answer
- last_evaluation
- UI toggles (sources/chunks/media)

---

## 15. Sidebar Settings

Adjust:

- llm_model
- embedding_model
- top_k_retrieval
- chunk_size
- chunk_overlap

Buttons:

- Clear chat
- Clear DB

---

## 16. Chat Pipeline

User → "**THINKING**" placeholder → backend runs → message replaced

---

## 17. RAG Toggle

When toggle is ON:

- calls `system.answer()`

When toggle is OFF:

- calls `system.noRAGAnswer()`

---

## 18. Answer Details Panel

Shows:

- Answer quality metrics
- Sources
- Retrieved chunks
- Related media

---

## 19. Conclusion

This is now a **multi-modal RAG pipeline**:

text ingestion  
URL ingestion  
image visual retrieval  
image OCR → searchable text  
video frame embedding  
video frame captioning with LLaVA  
evaluation + hallucination detection  
RAG toggle to compare RAG vs no-RAG  

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8cd13c",
   "metadata": {},
   "source": [
    "# Demo Notebook: Embeddings & Vector Search (Task 2)\n",
    "This notebook demonstrates dataset loading, chunking, embedding, storing, and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8a11ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead254c",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f92f1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Dataset loaded successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_text = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) significantly improves the accuracy and reliability of language models by grounding their answers in external knowledge sources.\n",
    "Traditional language models often hallucinate or confidently produce incorrect information because they cannot verify facts or access new knowledge.\n",
    "RAG solves this by connecting retrieval systems with generative models.\n",
    "First, relevant documents are retrieved using similarity search techniques.\n",
    "Then, the retrieved text is inserted into the prompt, allowing the model to generate answers based on real context instead of guessing.\n",
    "This approach is used in systems like support chatbots, academic research assistants, knowledge search tools, and customer service AI.\n",
    "It enables models to answer domain-specific questions such as university procedures, medical guidelines, or technical documentation.\n",
    "With embeddings and vector search, we can find semantically similar text even if exact wording differs.\n",
    "Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline.\n",
    "\n",
    "When writing queries for RAG systems, it is important to:\n",
    "- Be clear and concise\n",
    "- Use domain-specific keywords\n",
    "- Include context when possible\n",
    "- Avoid vague pronouns\n",
    "Effective query design improves retrieval quality and reduces the chance of irrelevant results.\n",
    "\n",
    "The pipeline often involve:\n",
    "1. Preprocessing datasets\n",
    "2. Chunking text into meaningful pieces\n",
    "3. Creating embeddings for each chunk\n",
    "4. Storing embeddings in a vector database\n",
    "5. Computing similarity between query and chunks\n",
    "6. Retrieving top-K chunks for LLM input\n",
    "\n",
    "RAG reduces hallucination by grounding LLM responses in retrieved context.\n",
    "Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text.\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Step 1: Dataset loaded successfully.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 1 failed:\", e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770c83a",
   "metadata": {},
   "source": [
    "## Step 2: Chunk the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99a18f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Created 13 meaningful chunks.\n",
      "\n",
      "Chunk 1:\n",
      "Retrieval-Augmented Generation (RAG) significantly improves the accuracy and reliability of language models by grounding their answers in external knowledge sources. Traditional language models often hallucinate or confidently produce incorrect information because they cannot verify facts or access new knowledge.\n",
      "\n",
      "Chunk 2:\n",
      "RAG solves this by connecting retrieval systems with generative models. First, relevant documents are retrieved using similarity search techniques.\n",
      "\n",
      "Chunk 3:\n",
      "Then, the retrieved text is inserted into the prompt, allowing the model to generate answers based on real context instead of guessing. This approach is used in systems like support chatbots, academic research assistants, knowledge search tools, and customer service AI.\n",
      "\n",
      "Chunk 4:\n",
      "It enables models to answer domain-specific questions such as university procedures, medical guidelines, or technical documentation. With embeddings and vector search, we can find semantically similar text even if exact wording differs.\n",
      "\n",
      "Chunk 5:\n",
      "Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline. When writing queries for RAG systems, it is important to: - Be clear and concise - Use domain-specific keywords - Include context when possible - Avoid vague pronouns Effective query design improves retrieval quality and reduces the chance of irrelevant results.\n",
      "\n",
      "Chunk 6:\n",
      "The pipeline often involve:\n",
      "\n",
      "Chunk 7:\n",
      "1. Preprocessing datasets\n",
      "\n",
      "Chunk 8:\n",
      "2. Chunking text into meaningful pieces\n",
      "\n",
      "Chunk 9:\n",
      "3. Creating embeddings for each chunk\n",
      "\n",
      "Chunk 10:\n",
      "4. Storing embeddings in a vector database\n",
      "\n",
      "Chunk 11:\n",
      "5. Computing similarity between query and chunks\n",
      "\n",
      "Chunk 12:\n",
      "6. Retrieving top-K chunks for LLM input\n",
      "\n",
      "Chunk 13:\n",
      "RAG reduces hallucination by grounding LLM responses in retrieved context. Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Split into lines, remove empty\n",
    "    lines = [line.strip() for line in dataset_text.splitlines() if line.strip()]\n",
    "    \n",
    "    # Merge 2â€“3 sentences per chunk, keep numbered lists intact\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        # Handle numbered list items as separate chunks\n",
    "        if line.strip().startswith(tuple(f\"{i}.\" for i in range(1, 20))):\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            chunks.append(line.strip())\n",
    "        else:\n",
    "            # Split line into sentences\n",
    "            sentences = re.split(r'(?<=[.!?]) +', line)\n",
    "            for s in sentences:\n",
    "                if current_chunk:\n",
    "                    current_chunk += \" \" + s\n",
    "                else:\n",
    "                    current_chunk = s\n",
    "                if current_chunk.count('.') >= 2:  # 2 sentences per chunk\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "    # Append leftover\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    print(f\"Step 2: Created {len(chunks)} meaningful chunks.\\n\")\n",
    "    for i, c in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}:\\n{c}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 2 failed:\", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f394b",
   "metadata": {},
   "source": [
    "## Step 3: Generate embeddings (requires sentence-transformers package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "357b2d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Model loaded successfully.\n",
      "Step 3: Embedded chunks successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    print(\"Step 2: Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Step 2 failed:\", e)\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    chunk_embeddings = model.encode(chunks, normalize_embeddings=True)\n",
    "    chunk_embeddings = np.array(chunk_embeddings).astype(\"float32\")\n",
    "    print(\"Step 3: Embedded chunks successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Embedding step failed:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717e3ba",
   "metadata": {},
   "source": [
    "## Step 4: FIASS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0da74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: FAISS index created and populated successfully.\n"
     ]
    }
   ],
   "source": [
    "dimension = chunk_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "try:\n",
    "    index.add(chunk_embeddings)\n",
    "    print(\"Step 4: FAISS index created and populated successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"FAISS insertion failed:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30566d3",
   "metadata": {},
   "source": [
    "## Step 5: Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500b252",
   "metadata": {},
   "source": [
    "## Step 6: Text retrived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b08b72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type your queries below. Type 'exit' to stop.\n",
      "\n",
      "\n",
      "Retrieved Top-K Chunks:\n",
      "\n",
      "No relevant chunks found above similarity threshold.\n",
      "\n",
      "\n",
      "Retrieved Top-K Chunks:\n",
      "\n",
      "Similarity=0.4678\n",
      "Chunk: Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline. When writing queries for RAG systems, it is important to: - Be clear and concise - Use domain-specific keywords - Include context when possible - Avoid vague pronouns Effective query design improves retrieval quality and reduces the chance of irrelevant results. \n",
      "\n",
      "Similarity=0.4184\n",
      "Chunk: RAG reduces hallucination by grounding LLM responses in retrieved context. Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text. \n",
      "\n",
      "Similarity=0.3504\n",
      "Chunk: RAG solves this by connecting retrieval systems with generative models. First, relevant documents are retrieved using similarity search techniques. \n",
      "\n",
      "Stopping query loop.\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "threshold = 0.2  # keep only relevant results\n",
    "\n",
    "print(\"\\nType your queries below. Type 'exit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"Query: \").strip()\n",
    "    \n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Stopping query loop.\")\n",
    "        break\n",
    "    \n",
    "    if not query:\n",
    "        print(\"Empty query. Try again.\")\n",
    "        continue\n",
    "    \n",
    "    query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "    query_embedding = np.array(query_embedding).astype(\"float32\")\n",
    "\n",
    "    similarities, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    print(\"\\nRetrieved Top-K Chunks:\\n\")\n",
    "    \n",
    "    found_any = False\n",
    "    for score, idx in zip(similarities[0], indices[0]):\n",
    "        if score >= threshold:\n",
    "            found_any = True\n",
    "            print(f\"Similarity={score:.4f}\")\n",
    "            print(\"Chunk:\", chunks[idx], \"\\n\")\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No relevant chunks found above similarity threshold.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

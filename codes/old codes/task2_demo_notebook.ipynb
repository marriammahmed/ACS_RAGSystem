{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de8cd13c",
   "metadata": {},
   "source": [
    "# Demo Notebook: Embeddings & Vector Search (Task 2)\n",
    "This notebook demonstrates dataset loading, chunking, embedding, storing, and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a11ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead254c",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f92f1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Dataset loaded successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_text = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) significantly improves the accuracy and reliability of language models by grounding their answers in external knowledge sources.\n",
    "Traditional language models often hallucinate or confidently produce incorrect information because they cannot verify facts or access new knowledge.\n",
    "RAG solves this by connecting retrieval systems with generative models.\n",
    "First, relevant documents are retrieved using similarity search techniques.\n",
    "Then, the retrieved text is inserted into the prompt, allowing the model to generate answers based on real context instead of guessing.\n",
    "This approach is used in systems like support chatbots, academic research assistants, knowledge search tools, and customer service AI.\n",
    "It enables models to answer domain-specific questions such as university procedures, medical guidelines, or technical documentation.\n",
    "With embeddings and vector search, we can find semantically similar text even if exact wording differs.\n",
    "Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline.\n",
    "\n",
    "When writing queries for RAG systems, it is important to:\n",
    "- Be clear and concise\n",
    "- Use domain-specific keywords\n",
    "- Include context when possible\n",
    "- Avoid vague pronouns\n",
    "Effective query design improves retrieval quality and reduces the chance of irrelevant results.\n",
    "\n",
    "The pipeline often involve:\n",
    "1. Preprocessing datasets\n",
    "2. Chunking text into meaningful pieces\n",
    "3. Creating embeddings for each chunk\n",
    "4. Storing embeddings in a vector database\n",
    "5. Computing similarity between query and chunks\n",
    "6. Retrieving top-K chunks for LLM input\n",
    "\n",
    "RAG reduces hallucination by grounding LLM responses in retrieved context.\n",
    "Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text.\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Step 1: Dataset loaded successfully.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 1 failed:\", e)\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770c83a",
   "metadata": {},
   "source": [
    "## Step 2: Chunk the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a18f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Created 13 meaningful chunks.\n",
      "\n",
      "Chunk 1:\n",
      "Retrieval-Augmented Generation (RAG) significantly improves the accuracy and reliability of language models by grounding their answers in external knowledge sources. Traditional language models often hallucinate or confidently produce incorrect information because they cannot verify facts or access new knowledge.\n",
      "\n",
      "Chunk 2:\n",
      "RAG solves this by connecting retrieval systems with generative models. First, relevant documents are retrieved using similarity search techniques.\n",
      "\n",
      "Chunk 3:\n",
      "Then, the retrieved text is inserted into the prompt, allowing the model to generate answers based on real context instead of guessing. This approach is used in systems like support chatbots, academic research assistants, knowledge search tools, and customer service AI.\n",
      "\n",
      "Chunk 4:\n",
      "It enables models to answer domain-specific questions such as university procedures, medical guidelines, or technical documentation. With embeddings and vector search, we can find semantically similar text even if exact wording differs.\n",
      "\n",
      "Chunk 5:\n",
      "Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline. When writing queries for RAG systems, it is important to: - Be clear and concise - Use domain-specific keywords - Include context when possible - Avoid vague pronouns Effective query design improves retrieval quality and reduces the chance of irrelevant results.\n",
      "\n",
      "Chunk 6:\n",
      "The pipeline often involve:\n",
      "\n",
      "Chunk 7:\n",
      "1. Preprocessing datasets\n",
      "\n",
      "Chunk 8:\n",
      "2. Chunking text into meaningful pieces\n",
      "\n",
      "Chunk 9:\n",
      "3. Creating embeddings for each chunk\n",
      "\n",
      "Chunk 10:\n",
      "4. Storing embeddings in a vector database\n",
      "\n",
      "Chunk 11:\n",
      "5. Computing similarity between query and chunks\n",
      "\n",
      "Chunk 12:\n",
      "6. Retrieving top-K chunks for LLM input\n",
      "\n",
      "Chunk 13:\n",
      "RAG reduces hallucination by grounding LLM responses in retrieved context. Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Split into lines, remove empty\n",
    "    lines = [line.strip() for line in dataset_text.splitlines() if line.strip()]\n",
    "    \n",
    "    # Merge 2â€“3 sentences per chunk, keep numbered lists intact\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        # Handle numbered list items as separate chunks\n",
    "        if line.strip().startswith(tuple(f\"{i}.\" for i in range(1, 20))):\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = \"\"\n",
    "            chunks.append(line.strip())\n",
    "        else:\n",
    "            # Split line into sentences\n",
    "            sentences = re.split(r'(?<=[.!?]) +', line)\n",
    "            for s in sentences:\n",
    "                if current_chunk:\n",
    "                    current_chunk += \" \" + s\n",
    "                else:\n",
    "                    current_chunk = s\n",
    "                if current_chunk.count('.') >= 2:  # 2 sentences per chunk\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "    # Append leftover\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    print(f\"Step 2: Created {len(chunks)} meaningful chunks.\\n\")\n",
    "    for i, c in enumerate(chunks):\n",
    "        print(f\"Chunk {i+1}:\\n{c}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 2 failed:\", e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f394b",
   "metadata": {},
   "source": [
    "## Step 3: Generate embeddings (requires sentence-transformers package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "357b2d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Sentence Transformer model loaded.\n",
      "\n",
      "Step 3: Embeddings generated for 13 chunks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"Step 3: Sentence Transformer model loaded.\\n\")\n",
    "    \n",
    "    chunk_embeddings = model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    print(f\"Step 3: Embeddings generated for {len(chunks)} chunks.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 3 failed:\", e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799ed141",
   "metadata": {},
   "source": [
    "## Step 3:FIASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0da74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: FAISS index created and 13 embeddings added.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dimension = chunk_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  \n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"Step 4: FAISS index created and {len(chunks)} embeddings added.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 4 failed:\", e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30566d3",
   "metadata": {},
   "source": [
    "## Step 5:Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0110faa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Query transformed into embedding: 'How does RAG reduce hallucination?'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = \"How does RAG reduce hallucination?\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    print(f\"Step 5: Query transformed into embedding: '{query}'\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 5 failed:\", e)\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe59590",
   "metadata": {},
   "source": [
    "## Step 6: Text retrived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b08b72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does RAG reduce hallucination?\n",
      "\n",
      "Retrieved Top-4 Chunks:\n",
      "\n",
      "Result 1 (score=0.529):\n",
      "RAG reduces hallucination by grounding LLM responses in retrieved context. Using retrieval-augmented generation, language models are less likely to hallucinate because they base answers on real text.\n",
      "\n",
      "Result 2 (score=0.264):\n",
      "Therefore, chunking, embedding, storage, and cosine similarity are essential building blocks for a working RAG pipeline. When writing queries for RAG systems, it is important to: - Be clear and concise - Use domain-specific keywords - Include context when possible - Avoid vague pronouns Effective query design improves retrieval quality and reduces the chance of irrelevant results.\n",
      "\n",
      "Result 3 (score=0.224):\n",
      "Retrieval-Augmented Generation (RAG) significantly improves the accuracy and reliability of language models by grounding their answers in external knowledge sources. Traditional language models often hallucinate or confidently produce incorrect information because they cannot verify facts or access new knowledge.\n",
      "\n",
      "Result 4 (score=0.216):\n",
      "RAG solves this by connecting retrieval systems with generative models. First, relevant documents are retrieved using similarity search techniques.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    top_k = 5\n",
    "    threshold = 0.1  # Minimum cosine similarity\n",
    "\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    retrieved_chunks = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if score >= threshold:\n",
    "            retrieved_chunks.append((score, chunks[idx]))\n",
    "\n",
    "    if not retrieved_chunks:\n",
    "        print(f\"Query: {query}\\n\\nNo relevant chunks found for this query.\\n\")\n",
    "    else:\n",
    "        print(f\"Query: {query}\\n\\nRetrieved Top-{len(retrieved_chunks)} Chunks:\\n\")\n",
    "        for i, (score, chunk) in enumerate(retrieved_chunks):\n",
    "            print(f\"Result {i+1} (score={score:.3f}):\\n{chunk}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Step 6 failed:\", e)\n",
    "    sys.exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
